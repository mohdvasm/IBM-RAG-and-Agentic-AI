Hello, and welcome to the video on multimodal chatbots and QA systems. After watching this video, you will be able to define multimodal AI systems and explain their impact on human-computer interaction, identify practical applications, and analyze real-world examples of multimodal AI, and demonstrate basic implementation concepts using frameworks like IBM Watson X. Imagine asking your phone to find a recipe, not just with words, but by showing it a picture of ingredients in your fridge. It understands your voice, the images you show, and even asks follow-up questions. This isn't sci-fi, it's the power of multimodal AI in action. So what are multimodal chatbots and QA, or question-answering systems? These are advanced AI applications that can process, understand, and generate responses based on multiple types of data inputs, such as text, images, audio, and sometimes video. Unlike traditional chatbots that only process text, these systems can see, read, and understand the world more like humans do. The block diagram shows the architecture of a multimodal AI system. The system accepts multiple input modalities, including text that includes queries, commands, conversations, etc., images, including photos, diagrams, etc., audio, such as voice commands and ambient sounds, and video, which is motion-based visual content. In the block diagram, you can see that the system is taking multiple inputs as images and text. The system processes each input modality separately, then fuses them across modalities to build a unified understanding. For example, answering a question about an image combines text and visual analysis to generate a context-aware response. Finally, based on the integrated understanding, the system generates appropriate responses, ranging from text answers and suggestions to action prompts or even image generation. Now let's look at a basic implementation of a multimodal QA system using the LLAMA 3.2 90B Vision Instruct model through IBM's Watson X platform. Before building your multimodal chatbot, the first step is to bring in all the necessary tools through Python libraries. The following code can be used for that purpose. You'll begin by establishing a connection to the AI service and configuring your chosen model. First, you'll create a credentials object with your URL and API key for authentication. The API client uses these credentials to establish a secure connection to IBM services. Next, specify which AI model you want to use through the model ID parameter. In this case, Meta's LLAMA 3.2 Vision model is being used. You'll also provide a project ID, which helps organize the API usage within IBM's platform. You'll initialize text chat parameters with default settings. You may customize parameters like temperature to control how creative or deterministic the model's responses should be. Finally, bring all these components together by creating a model inference object, which serves as your interface to the model. Before image analysis, convert images into formats that the model can process. Since AI can't see like humans, it needs numerical or text-based representations. Create two functions to handle this conversion, based on the image source. The prepareImage function handles local image files stored on your system. It opens the image file in binary mode, reads the raw data, and then converts this binary information into a Base64 encoded string. This encoding transforms the image into a lengthy text string that preserves all the visual information while making it compatible with text-based APIs. For images available online, create a prepareImageFromURL function that first downloads the image using an HTTP request and then performs the same Base64 encoding. This approach gives you the flexibility to work with images regardless of whether they're stored locally or online. At the core of your implementation is the queryMultimodalModel function, which combines text and images into a format the model can process together. This function creates a specific message structure expected by the API, with each message containing both text and image elements carefully arranged. When you call model chat with a structured message, both text and image are sent together. The model processes them in context, enabling it to reference visual details directly in response to your question. After processing, extract just the text response from a potentially more complex return object, providing a clear answer to present to the user. With all the components in place, you're now ready to use your multimodal question answering system. You'll start by preparing an image using one of the encoding functions from step 3. This gives you the Base64 encoded string representation of your image that the model can process. Next, craft a specific question about the image. This could be a general inquiry like, what can you see in this image? Or something more specific like, is there anything unsafe in this workplace photo? The question guides the model's attention to particular aspects of the image that interest us.
Play video starting at :6:3 and follow transcript6:03
To further shape how the model approaches your question, you can create a system prompt. This optional text sets the context for how the model should respond. For instance, telling it to act as an expert nutritionist analyzing food will produce very different responses than, act as a fashion consultant giving style advice, even when looking at the same image. This prompt essentially gives the model a role or perspective to adopt. You'll then call your function with all three components, the encoded image, your specific question, and the system prompt. The model processes this combined input and returns a natural language response that addresses your question, based specifically on what it sees in the image. The result is a contextually relevant answer that seamlessly blends an understanding of both the visual content and the textual query. In this video, you learned that multimodal chatbots and QA systems are advanced AI systems that process and respond to multiple data types, such as text, images, audio, and video. Unlike traditional text chatbots, these systems can see, read, and understand the world more like humans do. Multimodal chatbots' key features include multiple input modalities, integrated understanding, and contextual response generation. The basic implementation steps of a multimodal QA system are setting up the environment and importing libraries, initializing the model, preparing an image for processing, creating the multimodal query function, and using the multimodal QA function.