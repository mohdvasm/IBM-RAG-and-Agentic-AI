Reading: Course Overview
Welcome to the Build Multimodal Generative AI Applications course!

This intermediate-level course is designed for AI developers, data scientists, and engineers looking to expand their expertise in building applications that leverage multimodal generative AI. You’ll gain hands-on experience integrating text, speech, images, and video to develop practical, cross-modal applications using powerful models and frameworks.

Throughout this course, you’ll explore how to build applications that transform and generate across modalities—such as turning speech into text, text into images, or images into captions—using models like Meta’s Llama 4, OpenAI Whisper, and DALL-E. You’ll work with tools such as Gradio, Flask, LangChain, and Hugging Face to prototype and deploy user-facing multimodal AI systems.

You’ll begin by mastering the fundamentals of multimodal AI, gaining experience with speech-to-text and text-to-speech applications. Then, you’ll learn how to generate and caption images and videos using advanced generative models. Finally, you’ll build real-world multimodal applications—such as chatbots, personal assistants, and recommendation systems—using cross-modal retrieval and generation techniques.

This course is part of the 
IBM RAG and Agentic AI Professional Certificate
, aimed at equipping learners with cutting-edge skills to develop next-generation AI systems that understand and generate multiple forms of media.

Prerequisites
To succeed in this course, you should have:

Proficiency in Python

Prior exposure to Flask, Gradio, and LangChain

A working knowledge of generative AI fundamentals and frameworks like Hugging Face

Learning objectives
After completing this course, you will be able to:

Explain key challenges and integration techniques in multimodal AI

Build applications using models like Whisper, DALL-E, and Llama 4

Generate and caption media across text, image, speech, and video modalities

Implement chatbots, recommendation engines, and other applications that use multimodal retrieval

Deploy AI applications using Flask and Gradio

Course Modules
Module 1: Foundations of Multimodal AI
Introduction to Multimodal AI

Text-to-Speech Technologies

Speech-to-Text Technologies

Build applications based on AI-generated audiobooks and automated meeting assistants using tools such as OpenAI Whisper, Mixtral, gTTS, Gradio, and LangChain

Module 2: Integrating Visual and Video Modalities
Understanding Image Captioning with Meta's Llama

Text-to-Video Generation with OpenAI's Sora

Introduction to image-to-text and text-to-video capabilities

Hands-on labs with image generation using DALL-E and image captioning using IBM watsonx and Granite

Module 3: Advanced Multimodal Applications
Introduction to multimodal retrieval-augmented generation (MM-RAG) 

Multimodal Chatbots and QA Systems

Build AI apps for fashion matching, visual search, and diet recommendation

Use Flask and Gradio to deploy interactive multimodal applications

Tools and Frameworks
You’ll gain experience with:

DALL-E, Whisper, and Llama 4 for multimodal generation

Gradio and Flask for building user interfaces

LangChain for chaining multimodal processing tasks

Hugging Face for deploying and accessing pre-trained models

Tips for Success
Stay hands-on: Engage deeply with labs and projects

Think creatively: Use multimodal AI in real-world applications

Leverage communities and documentation for open-source tools

Ready to explore how AI sees, hears, speaks, and creates across modalities? Let’s begin your journey into multimodal generative AI.

